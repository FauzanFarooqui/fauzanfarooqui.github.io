---
layout: archive
title: "Publication Work"
permalink: /publications/
author_profile: true
---

My research work resulted in the following outputs (the first two listed are from my undergrad, currently under review; another was accepted at NLLP@EMNLP'23).

- **Open Information Extraction** (In-review)\
    _Co-first-author, long paper_ \
    _2023_
    - Proposed novel embedding methods that increase performance by 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores, being the first to integrate helpful external features with a Seq2Seq LLM for the task.
    - Contributed a synthetic dataset that boosted performance by 73.7% and 37.9% on Recall and F1 scores over the Seq2Seq version of the current largest annotated dataset, the latter which we show to be very flawed.
    - First to use a feature that significantly reduces compute resources by using 72% less tags compared to its counterpart, while maintaining the same performance boost!
    - Extended Amazon's work on structured prediction tasks to include OIE, thus being the first to study how SP pre-training affects OIE performance.
    - This work was supervised by Dr. Mansi Radke.
    
- **Open Link Prediction in Open Knowledge Graphs** (In-review) \
    _First author, Short paper_ \
    _2023_
    - Investigated how Graph Neural Networks can enhance entity-mention node vectors to improve reasoning over sparse Open KGs.
    - Investigated the effect of the hashing trick and FP-16 precision
    - Conducted ablation studies with semantic node features

- **Testing LLMs in the Legal Domain** (Accepted: pre-print coming soon!) \
  _Venue: Natural Legal Language Processing Workshop, co-located with EMNLP 2023_ \
  _Short workshop paper_
  - Benchmarked performance of various LLMs over the LEDGAR dataset by zero-shot prompting.
